{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of how to use `torch.optim.SGD` \n",
    " + The Rosenbrock function is minimized\n",
    " + In a similar way `torch.optim.Adam` could be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import scipy.optimize as so\n",
    "\n",
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    \"\"\"Rosenbrock function.\"\"\"\n",
    "    return (1-x[0])**2 + 100*(x[1] - x[0]**2)**2\n",
    "\n",
    "def rosenbrock_contour(iterates=None):\n",
    "    \"\"\"Plot contours of the Rosenbrock function.\"\"\"\n",
    "    n = 250\n",
    "    X, Y = np.meshgrid(np.linspace(-2,2,n), \n",
    "                       np.linspace(-1,3,n))\n",
    "    fig = plt.figure(figsize=(14,8))\n",
    "    plt.contour(X, Y, rosenbrock([X,Y]), np.logspace(-0.5, 3.5, 20, base=10), cmap='gray')\n",
    "\n",
    "    if iterates is not None: \n",
    "        if isinstance(iterates, dict):\n",
    "            for key, value in iterates.items():\n",
    "                plt.plot(*(zip(*value)), ls='--', marker='o', label='lr: {}'.format(key))\n",
    "        else:\n",
    "            plt.plot(*(zip(*iterates)), 'bo--')\n",
    "            \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    if isinstance(iterates, dict): plt.legend()\n",
    "    \n",
    "x0 = np.array([-1.9, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(lr=1e-3, mu=0.0, nesterov=False, use_torch_opt=False, N=500, model=rosenbrock):\n",
    "    \"\"\"Gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    lr : :obj:`float`\n",
    "        Learning rate.\n",
    "\n",
    "    mu : :obj:`float` [0, 1)\n",
    "        Momentum, see [1] (1-2).\n",
    "\n",
    "    nesterov : :obj:`bool`\n",
    "        If `True` use Nesterov accelerated gradient,\n",
    "        see [1] (3-4).\n",
    "\n",
    "    use_torch_opt : :obj:`bool`\n",
    "        If `True` use torch.optim.SGD. Note that there are \n",
    "        slight differences in the interpretation of the \n",
    "        `lr` and `mu` parameters (see the documentation of optim.SGD),\n",
    "        hence the results would be different from the manual version.\n",
    "\n",
    "    N : :obj:`int`\n",
    "        Number of iterations.\n",
    "\n",
    "    model : :obj:`callable`\n",
    "        The model.\n",
    "        \n",
    "    References\n",
    "    -----------\n",
    "    [1] http://proceedings.mlr.press/v28/sutskever13.pdf\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    :obj:`list(numpy.array)`\n",
    "        The iterates.\n",
    "    \"\"\"\n",
    "    x = torch.tensor(x0, requires_grad=True)\n",
    "    \n",
    "    opt = None\n",
    "    if use_torch_opt:\n",
    "        opt = optim.SGD([x], lr=lr, momentum=mu, nesterov=nesterov)\n",
    "        \n",
    "    iterates = [x.clone().data.numpy()]\n",
    "    v = torch.zeros(len(x)).double()\n",
    "    for k in range(N):\n",
    "        if opt is None:  # perform step manually\n",
    "            if nesterov: y = model(x + mu * v)\n",
    "            else: y = model(x)\n",
    "            \n",
    "            y.backward()\n",
    "            with torch.no_grad():\n",
    "                v = mu * v - lr * x.grad\n",
    "                x += v\n",
    "                x.grad.zero_()\n",
    "        else:\n",
    "            model(x).backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        iterates.append(x.clone().data.numpy())\n",
    "\n",
    "    return iterates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powell method\n",
    " + just for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosenbrock_contour(so.fmin_powell(rosenbrock, x0=x0, retall=True)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [1e-3/2, 3*1e-3/2, 2*1e-3]  # learning rates to test with\n",
    "\n",
    "rosenbrock_contour({round(lr, 4): gd(lr) for lr in lrs})\n",
    "rosenbrock_contour({round(lr, 4): gd(lr, use_torch_opt=True) for lr in lrs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosenbrock_contour(gd(lr=1e-3/2, mu=0.9))\n",
    "rosenbrock_contour(gd(lr=1e-3/2, mu=0.9, use_torch_opt=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov accelerated gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rosenbrock_contour(gd(lr=1e-3/2, mu=0.9, nesterov=True))\n",
    "rosenbrock_contour(gd(lr=1e-3/2, mu=0.9, nesterov=True, use_torch_opt=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
