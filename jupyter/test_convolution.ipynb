{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "sys.path.append('..')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = utils.get_mnist('../data')\n",
    "\n",
    "bs = 3\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a batch from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_dl)  # an iterator\n",
    "xb, yb = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a batch of 3 samples (each sample has size 28*28)\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the size of the samples in the batch\n",
    " + there are 3 samples in the batch\n",
    " + there is 1 input channel\n",
    " + each image has size 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = xb.view(-1, 1, 28, 28)\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a convolution object\n",
    " + `in_channels`: number of image channels (3 for RGB, 1 for Grayscale)\n",
    " + `out_channels`: number of different masks to apply to each input channel\n",
    " + `kernel_size`: size of the square kernel\n",
    " + `padding`: number of zeros added on all sides of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = 3\n",
    "conv = torch.nn.Conv2d(1, 5, kernel_size=ks, stride=1, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the weights and bias for the k-th mask (initialized with random values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "conv.weight[k].data, conv.bias[k].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply a convolution to the samples of the batch.\n",
    " + there are 3 samples in the batch\n",
    " + there are 5 output channels\n",
    " + each image has size 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conv(xb)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the output channels for all samples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the original images in the batch\n",
    "utils.show_random_samples(xb, rows=1, cols=bs, shuffle=False)\n",
    "\n",
    "# plot the result of the convolution\n",
    "for k in range(bs):\n",
    "    utils.show_random_samples(c[k].detach(), rows=1, cols=conv.out_channels, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example from http://cs231n.github.io/convolutional-networks/\n",
    " + When there are three input layers and two output layers, there are:\n",
    "  + 3 weighting matrices for each output (hence 6 in total)\n",
    "  + 1 bias vector per output (hence 2 in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_convolution(x0, x1, x2, w0, w1, w2, b):\n",
    "    \"\"\"Apply convolution (explicit) example.\n",
    "    \n",
    "    x0, x1, x2: 7x7 images (three channels)\n",
    "    w0, w1, w2: 3x3 kernels\n",
    "    b: bias\n",
    "    \"\"\"\n",
    "    n = 3  # kernel size\n",
    "    stride = 2\n",
    "    out = np.ndarray((3,3), dtype=np.int)\n",
    "\n",
    "    k1 = 0\n",
    "    for i in range(0, 6, stride):\n",
    "        k2 = 0\n",
    "        for j in range(0, 6, stride):\n",
    "            out[k1,k2] = np.sum(x0[i:i+n, j:j+n] * w0 + \n",
    "                                x1[i:i+n, j:j+n] * w1 + \n",
    "                                x2[i:i+n, j:j+n] * w2) + b\n",
    "            out[k1,k2] = np.sum(x0[i:i+n, j:j+n] * w0 + \n",
    "                                x1[i:i+n, j:j+n] * w1 + \n",
    "                                x2[i:i+n, j:j+n] * w2) + b\n",
    "            out[k1,k2] = np.sum(x0[i:i+n, j:j+n] * w0 + \n",
    "                                x1[i:i+n, j:j+n] * w1 + \n",
    "                                x2[i:i+n, j:j+n] * w2) + b\n",
    "            k2 += 1\n",
    "        k1 += 1\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([[0,0,0,0,0,0,0], [0,1,0,1,2,2,0], [0,2,2,2,0,2,0], \n",
    "               [0,0,0,0,0,1,0], [0,1,2,0,1,0,0], [0,1,1,2,1,1,0], [0,0,0,0,0,0,0]])\n",
    "\n",
    "x1 = np.array([[0,0,0,0,0,0,0], [0,1,0,2,2,0,0], [0,1,1,2,2,0,0], \n",
    "               [0,0,0,2,2,2,0], [0,2,1,2,0,2,0], [0,1,0,0,2,2,0], [0,0,0,0,0,0,0]])\n",
    "\n",
    "x2 = np.array([[0,0,0,0,0,0,0], [0,0,0,2,0,0,0], [0,1,2,2,0,1,0], \n",
    "               [0,1,1,0,1,0,0], [0,2,0,1,2,2,0],  [0,2,0,0,1,0,0], [0,0,0,0,0,0,0]])\n",
    "\n",
    "b0 = 1\n",
    "w00 = np.array([[0,0,-1], [1,1,1], [0,1,0]])\n",
    "w01 = np.array([[1,1,0], [0,0,0], [1,1,0]])\n",
    "w02 = np.array([[-1,1,-1], [0,0,1], [1,-1,1]])\n",
    "\n",
    "b1 = 0\n",
    "w10 = np.array([[1,1,0], [1,-1,0], [-1,-1,-1]])\n",
    "w11 = np.array([[-1,-1,1], [0,-1,1], [1,1,1]])\n",
    "w12 = np.array([[-1,-1,1], [0,0,1], [0,-1,-1]])\n",
    "\n",
    "X = torch.FloatTensor([x0,x1,x2]).view(-1,3,7,7)\n",
    "conv = torch.nn.Conv2d(3, 2, kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "conv.weight[0] = torch.tensor([w00, w01, w02])\n",
    "conv.weight[1] = torch.tensor([w10, w11, w12])\n",
    "conv.bias[0] = torch.tensor([b0])\n",
    "conv.bias[1] = torch.tensor([b1])\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# perform convolution manually\n",
    "print(apply_convolution(x0, x1, x2, w00, w01, w02, b0))\n",
    "print(apply_convolution(x0, x1, x2, w10, w11, w12, b1))\n",
    "\n",
    "# perform convolution using pytorch\n",
    "conv(X).detach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
